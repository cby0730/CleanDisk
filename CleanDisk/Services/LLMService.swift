import Foundation
import MLX
import MLXLMCommon
import MLXLLM

/// æ”¯æ´çš„ LLM æ¨¡å‹
enum LLMModel: String, CaseIterable, Identifiable {
    case deepSeekR1 = "mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-8bit"
    case llama32_3B = "mlx-community/Llama-3.2-3B-Instruct-4bit"
    case qwen25_3B = "mlx-community/Qwen2.5-3B-Instruct-8bit"
    case qwen3_4B = "mlx-community/Qwen3-4B-4bit"
    case qwen3_4B_2507 = "mlx-community/Qwen3-4B-Instruct-2507-4bit"
    case llama3_8B = "mlx-community/Meta-Llama-3-8B-Instruct-4bit"
    case qwen3_8B = "mlx-community/Qwen3-8B-4bit"
    case gptOss20B = "mlx-community/gpt-oss-20b-MXFP4-Q8"
    
    var id: String { rawValue }
    
    /// é¡¯ç¤ºåç¨±
    var displayName: String {
        switch self {
        case .deepSeekR1: return "DeepSeek R1 1.5B (æœ€å¿«)"
        case .llama32_3B: return "Llama 3.2 3B"
        case .qwen25_3B: return "Qwen 2.5 3B (æ¨è–¦)"
        case .qwen3_4B: return "Qwen 3 4B"
        case .qwen3_4B_2507: return "Qwen 3 4B (2507)"
        case .llama3_8B: return "Llama 3 8B"
        case .qwen3_8B: return "Qwen 3 8B"
        case .gptOss20B: return "GPT-OSS 20B (æœ€æº–ç¢º)"
        }
    }
    
    /// é ä¼°å¤§å°
    var estimatedSize: String {
        switch self {
        case .deepSeekR1: return "~1.5 GB"
        case .llama32_3B: return "~2 GB"
        case .qwen25_3B: return "~3 GB"
        case .qwen3_4B, .qwen3_4B_2507: return "~2.5 GB"
        case .llama3_8B, .qwen3_8B: return "~5 GB"
        case .gptOss20B: return "~12 GB"
        }
    }
    
    /// é€Ÿåº¦è©•ç´šï¼ˆ1-5ï¼Œ5æœ€å¿«ï¼‰
    var speedRating: Int {
        switch self {
        case .deepSeekR1: return 5
        case .llama32_3B, .qwen25_3B: return 4
        case .qwen3_4B, .qwen3_4B_2507: return 3
        case .llama3_8B, .qwen3_8B: return 2
        case .gptOss20B: return 1
        }
    }
}

/// AI åˆªé™¤å»ºè­°çµæœ
struct FileDeletionSuggestion {
    let shouldDelete: Bool
    let confidence: Confidence
    let reasoning: String
    
    enum Confidence: String {
        case high = "é«˜"
        case medium = "ä¸­"
        case low = "ä½"
    }
    
    var icon: String {
        shouldDelete ? "âŒ" : "âœ…"
    }
    
    var recommendation: String {
        shouldDelete ? "å»ºè­°åˆªé™¤" : "å»ºè­°ä¿ç•™"
    }
}

/// LLM æœå‹™ï¼Œç®¡ç† MLX æ¨¡å‹çš„ç”Ÿå‘½é€±æœŸå’Œ AI å»ºè­°ç”Ÿæˆ
@MainActor
class LLMService: ObservableObject {
    // MARK: - Published State
    
    @Published var isModelLoaded: Bool = false
    @Published var isGenerating: Bool = false
    @Published var loadingProgress: String = ""
    @Published var error: String?
    @Published var selectedModel: LLMModel {
        didSet {
            // ç•¶æ¨¡å‹æ”¹è®Šæ™‚ï¼Œä¿å­˜åˆ° UserDefaults
            UserDefaults.standard.set(selectedModel.rawValue, forKey: "selectedLLMModel")
            // å¦‚æœæœ‰æ¨¡å‹å·²è¼‰å…¥ï¼Œéœ€è¦é‡æ–°è¼‰å…¥æ–°æ¨¡å‹
            if isModelLoaded {
                Task {
                    await reloadModel()
                }
            }
        }
    }
    
    // MARK: - Private Properties
    
    private var model: ModelContainer?
    private var currentLoadedModel: LLMModel?
    
    // MARK: - Initialization
    
    init() {
        // å¾ UserDefaults è¼‰å…¥ä¸Šæ¬¡çš„æ¨¡å‹é¸æ“‡
        if let savedModelId = UserDefaults.standard.string(forKey: "selectedLLMModel"),
           let savedModel = LLMModel(rawValue: savedModelId) {
            self.selectedModel = savedModel
        } else {
            // é è¨­ä½¿ç”¨ Qwen 2.5 3Bï¼ˆå¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºåº¦ï¼‰
            self.selectedModel = .qwen25_3B
        }
    }
    
    // MARK: - Model Lifecycle
    
    /// è¼‰å…¥ MLX æ¨¡å‹
    func loadModel() async {
        guard !isModelLoaded else { return }
        
        isGenerating = true
        loadingProgress = "æ­£åœ¨è¼‰å…¥ \(selectedModel.displayName)..."
        error = nil
        
        do {
            let modelFactory = LLMModelFactory.shared
            let configuration = ModelConfiguration(id: selectedModel.rawValue)
            
            // æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰
            let modelPath = FileManager.default.homeDirectoryForCurrentUser
                .appendingPathComponent("Library/Caches/models/mlx-community")
                .appendingPathComponent(selectedModel.rawValue.components(separatedBy: "/").last ?? "")
            
            let wasDownloaded = FileManager.default.fileExists(atPath: modelPath.path)
            
            if wasDownloaded {
                loadingProgress = "è¼‰å…¥æ¨¡å‹ä¸­..."
            } else {
                loadingProgress = "ä¸‹è¼‰æ¨¡å‹ä¸­ï¼ˆ\(selectedModel.estimatedSize)ï¼‰..."
            }
            
            // è¼‰å…¥æ¨¡å‹å®¹å™¨ï¼ˆå¦‚æœéœ€è¦æœƒå…ˆä¸‹è¼‰ï¼‰
            model = try await modelFactory.loadContainer(configuration: configuration)
            
            // å¦‚æœæ¨¡å‹æ˜¯å‰›ä¸‹è¼‰çš„ï¼Œé¡¯ç¤ºã€Œè¼‰å…¥æ¨¡å‹ä¸­ã€ç‹€æ…‹
            if !wasDownloaded {
                loadingProgress = "è¼‰å…¥æ¨¡å‹ä¸­..."
                // çµ¦ä½¿ç”¨è€…è‡³å°‘ 0.5 ç§’çœ‹åˆ°è¼‰å…¥ç‹€æ…‹
                try? await Task.sleep(nanoseconds: 500_000_000)
            }
            
            currentLoadedModel = selectedModel
            isModelLoaded = true
            loadingProgress = "æ¨¡å‹å·²å°±ç·’"
            print("âœ… LLM æ¨¡å‹è¼‰å…¥æˆåŠŸ: \(selectedModel.displayName)")
        } catch {
            self.error = "æ¨¡å‹è¼‰å…¥å¤±æ•—: \(error.localizedDescription)"
            loadingProgress = ""
            print("âŒ LLM æ¨¡å‹è¼‰å…¥å¤±æ•—: \(error)")
        }
        
        isGenerating = false
    }
    
    /// å¸è¼‰æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”
    func unloadModel() {
        model = nil
        currentLoadedModel = nil
        isModelLoaded = false
        loadingProgress = ""
        print("ğŸ—‘ï¸ LLM æ¨¡å‹å·²å¸è¼‰")
    }
    
    /// é‡æ–°è¼‰å…¥æ¨¡å‹ï¼ˆåˆ‡æ›æ¨¡å‹æ™‚ä½¿ç”¨ï¼‰
    private func reloadModel() async {
        print("ğŸ”„ åˆ‡æ›æ¨¡å‹: \(currentLoadedModel?.displayName ?? "ç„¡") â†’ \(selectedModel.displayName)")
        unloadModel()
        await loadModel()
    }
    
    // MARK: - AI Suggestion
    
    /// ç‚ºæŒ‡å®šæª”æ¡ˆç²å– AI åˆªé™¤å»ºè­°
    func getSuggestion(for fileNode: FileNode) async throws -> FileDeletionSuggestion {
        guard isModelLoaded, let model = model else {
            throw LLMError.modelNotLoaded
        }
        
        isGenerating = true
        loadingProgress = "AI åˆ†æä¸­..."
        
        // ç¢ºä¿è‡³å°‘é¡¯ç¤º 1 ç§’çš„åˆ†æä¸­ç‹€æ…‹
        let startTime = Date()
        
        defer {
            Task {
                let elapsedTime = Date().timeIntervalSince(startTime)
                if elapsedTime < 1.0 {
                    try? await Task.sleep(nanoseconds: UInt64((1.0 - elapsedTime) * 1_000_000_000))
                }
                await MainActor.run {
                    isGenerating = false
                    loadingProgress = ""
                }
            }
        }
        
        // å»ºç«‹ prompt
        let prompt = buildPrompt(for: fileNode)
        
        // ç”Ÿæˆå»ºè­°
        let response = try await generateResponse(prompt: prompt, model: model)
        
        // è§£æå›æ‡‰
        let suggestion = parseSuggestion(from: response, fileNode: fileNode)
        
        return suggestion
    }
    
    // MARK: - Private Helpers
    
    /// å»ºç«‹ AI prompt
    private func buildPrompt(for fileNode: FileNode) -> String {
        let modDate = fileNode.modificationDate.map {
            let formatter = RelativeDateTimeFormatter()
            formatter.unitsStyle = .full
            return formatter.localizedString(for: $0, relativeTo: Date())
        } ?? "æœªçŸ¥"
        
        return """
        ä½ æ˜¯ä¸€å€‹æª”æ¡ˆç³»çµ±å°ˆå®¶ï¼Œå¹«åŠ©ç”¨æˆ¶åˆ¤æ–·æª”æ¡ˆæ˜¯å¦æ‡‰è©²åˆªé™¤ã€‚
        
        æª”æ¡ˆè³‡è¨Šï¼š
        - åç¨±ï¼š\(fileNode.name)
        - è·¯å¾‘ï¼š\(fileNode.url.path)
        - å¤§å°ï¼š\(fileNode.formattedSize)
        - é¡å‹ï¼š\(fileNode.fileType)
        - å‰¯æª”åï¼š\(fileNode.fileExtension.isEmpty ? "ç„¡" : fileNode.fileExtension)
        - æœ€å¾Œä¿®æ”¹ï¼š\(modDate)
        - æ˜¯å¦éš±è—æª”ï¼š\(fileNode.isHidden ? "æ˜¯" : "å¦")
        
        è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šåˆ¤æ–·é€™å€‹æª”æ¡ˆæ˜¯å¦æ‡‰è©²åˆªé™¤ã€‚
        
        å›æ‡‰æ ¼å¼ï¼ˆJSONï¼‰ï¼š
        {
          "should_delete": true/false,
          "confidence": "high/medium/low",
          "reasoning": "ç°¡çŸ­èªªæ˜ï¼ˆç¹é«”ä¸­æ–‡ï¼Œ50å­—å…§ï¼‰"
        }
        
        åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–å…§å®¹ã€‚
        """
    }
    
    /// ç”Ÿæˆ LLM å›æ‡‰
    private func generateResponse(prompt: String, model: ModelContainer) async throws -> String {
        var fullResponse = ""
        
        try await model.perform { context in
            let input = try await context.processor.prepare(input: UserInput(prompt: prompt))
            
            let params = GenerateParameters(temperature: 0.3, topP: 0.9)
            let tokenStream = try generate(input: input, parameters: params, context: context)
            
            for await part in tokenStream {
                if let chunk = part.chunk {
                    fullResponse += chunk
                }
            }
        }
        
        return fullResponse
    }
    
    /// è§£æ LLM å›æ‡‰ç‚ºçµæ§‹åŒ–å»ºè­°
    private func parseSuggestion(from response: String, fileNode: FileNode) -> FileDeletionSuggestion {
        // å˜—è©¦è§£æ JSON
        if let jsonData = extractJSON(from: response),
           let parsed = try? JSONDecoder().decode(SuggestionResponse.self, from: jsonData) {
            
            let confidence: FileDeletionSuggestion.Confidence = {
                switch parsed.confidence.lowercased() {
                case "high", "é«˜": return .high
                case "medium", "ä¸­": return .medium
                default: return .low
                }
            }()
            
            return FileDeletionSuggestion(
                shouldDelete: parsed.should_delete,
                confidence: confidence,
                reasoning: parsed.reasoning
            )
        }
        
        // Fallback: ä½¿ç”¨å•Ÿç™¼å¼è¦å‰‡
        return fallbackSuggestion(for: fileNode, response: response)
    }
    
    /// å¾å›æ‡‰ä¸­æå– JSON
    private func extractJSON(from response: String) -> Data? {
        // å°‹æ‰¾ JSON ç‰©ä»¶
        guard let startIndex = response.firstIndex(of: "{"),
              let endIndex = response.lastIndex(of: "}") else {
            return nil
        }
        
        let jsonString = String(response[startIndex...endIndex])
        return jsonString.data(using: .utf8)
    }
    
    /// ç•¶ JSON è§£æå¤±æ•—æ™‚çš„ fallback å»ºè­°
    private func fallbackSuggestion(for fileNode: FileNode, response: String) -> FileDeletionSuggestion {
        print("âš ï¸ JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨ fallback é‚è¼¯")
        print("åŸå§‹å›æ‡‰ï¼š\(response)")
        
        // åŸºæ–¼æª”æ¡ˆé¡å‹çš„ç°¡å–®å•Ÿç™¼å¼
        let ext = fileNode.fileExtension.lowercased()
        let tempExtensions = ["tmp", "temp", "cache", "log", "bak", "old"]
        let shouldDelete = tempExtensions.contains(ext)
        
        let reasoning = shouldDelete
            ? "æ ¹æ“šå‰¯æª”ååˆ¤æ–·ï¼Œé€™å¯èƒ½æ˜¯è‡¨æ™‚æª”æ¡ˆ"
            : "ç„¡æ³•ç¢ºå®šï¼Œå»ºè­°æ‰‹å‹•æª¢æŸ¥å¾Œæ±ºå®š"
        
        return FileDeletionSuggestion(
            shouldDelete: shouldDelete,
            confidence: .low,
            reasoning: reasoning
        )
    }
}

// MARK: - Supporting Types

/// LLM å›æ‡‰çš„ JSON çµæ§‹
private struct SuggestionResponse: Codable {
    let should_delete: Bool
    let confidence: String
    let reasoning: String
}

/// LLM æœå‹™éŒ¯èª¤
enum LLMError: LocalizedError {
    case modelNotLoaded
    case generationFailed(String)
    
    var errorDescription: String? {
        switch self {
        case .modelNotLoaded:
            return "AI æ¨¡å‹å°šæœªè¼‰å…¥ï¼Œè«‹ç¨å€™"
        case .generationFailed(let message):
            return "AI ç”Ÿæˆå¤±æ•—ï¼š\(message)"
        }
    }
}
